{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzhan\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259122 54345\n"
     ]
    }
   ],
   "source": [
    "def present(idn):\n",
    "    d={}\n",
    "    for i,idn in enumerate(idn):\n",
    "        d[idn]=i\n",
    "    return d  \n",
    "#/my_data/\n",
    "train=pd.read_csv('/my_data/train.csv',index_col=0)\n",
    "validate=pd.read_csv('/my_data/validate.csv',index_col=0)\n",
    "\n",
    "# train=train[:50000]\n",
    "# validate=validate[:20000]\n",
    "\n",
    "users=sorted(list(set(train.index).union(set(validate.index))))\n",
    "nb_users=len(users)\n",
    "business=sorted(list(set(train['business_id']).union(set(validate['business_id']))))\n",
    "nb_business=len(business)\n",
    "print(nb_users,nb_business)\n",
    "dic=present(business)\n",
    "# Converting the data into an array with users in lines and movies in columns\n",
    "\n",
    "def convert(data,batch_size):\n",
    "    #new_data = []\n",
    "    #data=data.iloc[0:(nb_users//batch_size)*batch_size,:]\n",
    "    for i,id_users in enumerate(users):\n",
    "        \n",
    "        id_business = data['business_id'][data.index == id_users]\n",
    "        ind=list(map(lambda x:dic[x],id_business))\n",
    "        #id_business = list(map(lambda x:business.index(x),l))\n",
    "        id_ratings = data['stars'][data.index == id_users]\n",
    "        ratings = np.zeros(nb_business)\n",
    "        ratings[ind] = id_ratings\n",
    "        if i%batch_size==0:\n",
    "            if i!=0:\n",
    "                yield np.array(U)\n",
    "            U=[list(ratings)]\n",
    "        else:\n",
    "            U.append(list(ratings))\n",
    "        #new_data.append(list(ratings))\n",
    "    #return new_data\n",
    "        \n",
    "    \n",
    "train_set = convert(train,500)\n",
    "val_set = convert(validate,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM:\n",
    "    \n",
    "    def __init__(self,train_set,nb_users,nb_business,factor,batch_size,iter_num = 20,alpha = 0.1,Lambda = 0.1,epsilon = 0.01):\n",
    "        '''\n",
    "        initiate parameters\n",
    "        '''\n",
    "        self.train_set = train_set\n",
    "        self.nb_users=nb_users\n",
    "        self.nb_business=nb_business\n",
    "        self.factor = factor\n",
    "        self.batch_size=batch_size\n",
    "        self.batchs=nb_users//batch_size\n",
    "        self.alpha = alpha\n",
    "        self.iter_num = iter_num\n",
    "        self.Lambda = Lambda\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "    def build(self,train):\n",
    "        \n",
    "\n",
    "        #initiate latent factor matrix\n",
    "        self.decompose_p = tf.Variable(tf.ones([self.nb_business,self.factor]))\n",
    "        self.decompose_q = tf.Variable(tf.ones([self.factor,self.nb_users]))    \n",
    "        \n",
    "        # Placeholders\n",
    "\n",
    "        self.u_ind = tf.placeholder('int32', shape=[self.batch_size])\n",
    "        #i = tf.placeholder('int32')\n",
    "        self.y_rate = tf.placeholder('float32', shape=[self.batch_size, self.nb_business])\n",
    "        self.y_mask = tf.placeholder('float32', shape=[self.batch_size, self.nb_business])\n",
    "        #batch_q = tf.placeholder('float32', shape=[self.batch_size, self.factor])\n",
    "\n",
    "        batch_q=tf.gather(tf.transpose(self.decompose_q),self.u_ind)\n",
    "        self.pred_y_rate = tf.matmul(batch_q, tf.transpose(self.decompose_p))\n",
    "        loss_m = tf.squared_difference(self.y_rate, self.pred_y_rate)\n",
    "        self.loss = tf.reduce_sum(loss_m * self.y_mask)+self.Lambda*(tf.reduce_sum(tf.square(self.decompose_p))+tf.reduce_sum(tf.square(batch_q)))\n",
    "        optimizer = tf.train.AdamOptimizer(self.alpha)\n",
    "        train_op = optimizer.minimize(self.loss)\n",
    "                                                      \n",
    "\n",
    "        #training\n",
    "        config = tf.ConfigProto() \n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    \n",
    "        old_error = float('inf')\n",
    "        \n",
    "        tot_loss = 0.0\n",
    "        for epoch in range(self.iter_num):\n",
    "            train_set = convert(train,500)\n",
    "            t = time.time()\n",
    "            \n",
    "            for i,U in enumerate(train_set):\n",
    "                #with self.sess.as_default():\n",
    "                #    batch_q=tf.transpose(self.decompose_q).eval()[i*self.batch_size:(i+1)*self.batch_size,:]  \n",
    "                u_ind=list(range(i*self.batch_size,(i+1)*self.batch_size))\n",
    "                mask=deepcopy(U)\n",
    "                mask[mask!=0]=1\n",
    "                feed={self.y_rate:U,self.y_mask:mask,self.u_ind:u_ind}\n",
    "                _, loss = self.sess.run([train_op, self.loss], feed_dict = feed)\n",
    "                tot_loss += loss\n",
    "                #print (i)\n",
    "                avg_loss = tot_loss / (epoch*self.batchs+i+1)\n",
    "                if i%10==0:\n",
    "                    print (\"Epoch %d\\tLoss\\t%.2f\\tTime %dmin\" \\\n",
    "                    % (epoch, avg_loss, (time.time()-t)))\n",
    "            \n",
    "\n",
    "\n",
    "        print (\"Recommender is built!\")\n",
    "        \n",
    "    def test(self,val_set):\n",
    "        \"\"\"\n",
    "        :return performance on test set (Mean Square Root Error)\n",
    "        \"\"\"\n",
    "        tot_loss = 0.0\n",
    "        for i,U in enumerate(val_set):\n",
    "            u_ind=list(range(i*self.batch_size,(i+1)*self.batch_size))\n",
    "            mask=deepcopy(U)\n",
    "            mask[mask!=0]=1\n",
    "            feed={self.y_rate:U,self.y_mask:mask,self.u_ind:u_ind}\n",
    "            loss = self.sess.run(self.loss, feed_dict = self.get_feed_dict(self.reader.get_next_test()))\n",
    "            tot_loss += loss\n",
    "        return tot_loss / self.batchs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fzhan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0\tLoss\t107242.00\tTime 10min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-bd77c50abfda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlfm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLFM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_users\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_business\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-bcbc7e43320e>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mU\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[1;31m#with self.sess.as_default():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;31m#    batch_q=tf.transpose(self.decompose_q).eval()[i*self.batch_size:(i+1)*self.batch_size,:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-08763964870a>\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(data, batch_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid_users\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mid_business\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'business_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mid_users\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid_business\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#id_business = list(map(lambda x:business.index(x),l))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_evaluate_compare\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   3724\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3725\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3726\u001b[1;33m                         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3728\u001b[0m                 \u001b[1;31m# technically we could support bool dtyped Index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "lfm=LFM(train_set,nb_users,nb_business,factor=10,batch_size=500,iter_num = 10,alpha = 0.15)\n",
    "\n",
    "lfm.build(train)\n",
    "print (lfm.decompose_p)\n",
    "print (lfm.decompose_q)\n",
    "\n",
    "loss=lfm.test(val_set)\n",
    "print(loss)\n",
    "# ex = range(len(lfm.delta_error))\n",
    "# plt.figure(1)\n",
    "# plt.plot(ex,lfm.delta_error)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
